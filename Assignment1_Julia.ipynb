{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f9cf89",
   "metadata": {},
   "source": [
    "# Assignment 1 - Conditioning and Sensitivity\n",
    "\n",
    "Julia translation of MATLAB numerical analysis assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "using Pkg\n",
    "Pkg.add([\"Plots\", \"LinearAlgebra\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387553d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra\n",
    "gr()  # Use GR backend for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe3f42",
   "metadata": {},
   "source": [
    "## Examples and Demonstration\n",
    "\n",
    "### Converting to Single Precision\n",
    "\n",
    "Julia stores numbers by default in 64-bit double precision floating-point numbers. To demonstrate floating-point errors, we convert numbers to single-precision (`Float32`) and compare to double-precision results.\n",
    "\n",
    "### Demonstrating Cancellation Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating cancellation error\n",
    "a = rand()  # Random number in double precision\n",
    "b = a + 0.1\n",
    "\n",
    "diff_single = Float32(b) - Float32(a)\n",
    "println(\"Single precision difference: \", diff_single)\n",
    "println(\"Absolute error: \", abs(diff_single - 0.1))\n",
    "println(\"Relative error: \", abs(diff_single - 0.1) / 0.1)\n",
    "\n",
    "# Sweep over different differences\n",
    "diffs = 10.0 .^ (-16:-1)\n",
    "relerrs = zeros(length(diffs))\n",
    "\n",
    "for i in 1:length(diffs)\n",
    "    b = a + diffs[i]\n",
    "    diff_single = Float32(b) - Float32(a)\n",
    "    relerrs[i] = abs(diff_single - diffs[i]) / diffs[i]\n",
    "end\n",
    "\n",
    "plot(diffs, relerrs, \n",
    "     seriestype=:scatter, \n",
    "     xscale=:log10, yscale=:log10,\n",
    "     markersize=6, color=:red,\n",
    "     xlabel=\"|a-b| in double precision\",\n",
    "     ylabel=\"Relative error computing b-a in single precision\",\n",
    "     legend=false,\n",
    "     title=\"Cancellation Error Demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2d82c",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "### Part (a) - Forward Error Analysis\n",
    "\n",
    "**Central difference approximation:**\n",
    "\n",
    "$$D_f = \\frac{f(x+h) - f(x-h)}{2h} = f'(x)(1 + \\delta_t)$$\n",
    "\n",
    "where $\\delta_t = O(h^2)$ (truncation error).\n",
    "\n",
    "**Floating-point error analysis:**\n",
    "\n",
    "The computed result satisfies:\n",
    "\n",
    "$$\\hat{D}_f - f'(x) = O(h^2) + O\\left(\\frac{\\varepsilon_M}{h}\\right)$$\n",
    "\n",
    "**Absolute error bound:**\n",
    "\n",
    "$$|\\hat{D}_f - f'(x)| \\leq C_1 h^2 + C_2 \\frac{\\varepsilon_M}{h}$$\n",
    "\n",
    "**Optimal step size:**\n",
    "\n",
    "Minimizing the error bound:\n",
    "\n",
    "$$h_{\\text{optimal}} \\propto \\varepsilon_M^{1/3}$$\n",
    "\n",
    "For double precision: $h_{\\text{opt}} \\approx 6.06 \\times 10^{-6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26364e09",
   "metadata": {},
   "source": [
    "### Part (b) - Double Precision Central Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91613b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function and its derivative\n",
    "f(x) = exp(-x) - x * (2 - x)\n",
    "x0 = 2.0\n",
    "\n",
    "# Range of h values to investigate\n",
    "h = 10.0 .^ range(-1, -20, length=50)\n",
    "\n",
    "# Machine precision for double\n",
    "epsval = eps(Float64)\n",
    "println(\"Machine epsilon (Float64): \", epsval)\n",
    "\n",
    "# Compute central difference approximation\n",
    "centdiff = [(f(x0 + hh) - f(x0 - hh)) / (2 * hh) for hh in h]\n",
    "\n",
    "# True derivative: f'(x) = -exp(-x) - 2 + 2x\n",
    "# At x=2: f'(2) = -exp(-2) - 2 + 4 = 2 - exp(-2) ≈ 1.86466471676\n",
    "D_true = 2 - exp(-2)\n",
    "println(\"True derivative at x=2: \", D_true)\n",
    "\n",
    "# Compute errors\n",
    "abs_err = abs.(centdiff .- D_true)\n",
    "rel_err = abs_err ./ abs(D_true)\n",
    "\n",
    "# Theoretical error bound (with C₁=1, C₂=1)\n",
    "error_bound = h.^2 .+ epsval ./ h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5553d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "p1 = plot(h, rel_err, \n",
    "     seriestype=:scatter, \n",
    "     xscale=:log10, yscale=:log10,\n",
    "     markersize=5, color=:red,\n",
    "     label=\"Empirical error\",\n",
    "     xlabel=\"h\",\n",
    "     ylabel=\"Relative error\",\n",
    "     title=\"Central Difference at x₀=2 (Float64)\",\n",
    "     legend=:bottomright)\n",
    "\n",
    "plot!(p1, h, error_bound, \n",
    "      seriestype=:scatter, \n",
    "      markersize=5, color=:blue,\n",
    "      label=\"Theoretical bound\")\n",
    "\n",
    "# Add vertical line at machine epsilon\n",
    "vline!([epsval], linestyle=:dash, color=:yellow, \n",
    "       label=\"ε_M = $(round(epsval, sigdigits=4))\", linewidth=2)\n",
    "\n",
    "display(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6be132",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- The empirical error closely follows the theoretical bound\n",
    "- Floating-point error dominates for small h (large $\\varepsilon_M/h$)\n",
    "- Truncation error dominates for large h (large $h^2$)\n",
    "- Optimal h is around $10^{-5}$ to $10^{-6}$\n",
    "- Below $h \\approx 10^{-16}$, degeneracy occurs: $f(x_0+h) = f(x_0)$ in floating point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbd852",
   "metadata": {},
   "source": [
    "### Part (c) - Single Precision Central Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e424845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single precision (Float32) analysis\n",
    "f32(x) = exp(-x) - x * (2 - x)  # Julia will use Float32 arithmetic if input is Float32\n",
    "\n",
    "x0_32 = Float32(2.0)\n",
    "h_32 = Float32.(10.0 .^ range(-1, -20, length=50))\n",
    "\n",
    "# Machine precision for single\n",
    "eps_single = eps(Float32)\n",
    "println(\"Machine epsilon (Float32): \", eps_single)\n",
    "println(\"Machine epsilon (Float64): \", eps(Float64))\n",
    "\n",
    "# Compute central difference in single precision\n",
    "centdiff_single = [(f32(x0_32 + hh) - f32(x0_32 - hh)) / (2 * hh) for hh in h_32]\n",
    "\n",
    "# Convert to Float64 for error calculation\n",
    "centdiff_f64 = Float64.(centdiff_single)\n",
    "h_f64 = Float64.(h_32)\n",
    "\n",
    "# Errors\n",
    "abs_err_32 = abs.(centdiff_f64 .- D_true)\n",
    "rel_err_32 = abs_err_32 ./ abs(D_true)\n",
    "\n",
    "# Theoretical error bound for single precision\n",
    "error_bound_32 = 0.01 .* h_f64.^2 .+ 1e9 .* (eps(Float64) ./ h_f64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot single precision results\n",
    "p2 = plot(h_f64, rel_err_32, \n",
    "     seriestype=:scatter, \n",
    "     xscale=:log10, yscale=:log10,\n",
    "     markersize=5, color=:red,\n",
    "     label=\"Empirical error (Float32)\",\n",
    "     xlabel=\"h\",\n",
    "     ylabel=\"Relative error\",\n",
    "     title=\"Central Difference at x₀=2 (Float32)\",\n",
    "     legend=:bottomright,\n",
    "     ylims=(1e-10, 10))\n",
    "\n",
    "# Add vertical line at single precision epsilon\n",
    "vline!([eps_single], linestyle=:dash, color=:yellow, \n",
    "       label=\"ε_M (Float32) = $(round(eps_single, sigdigits=4))\", linewidth=2)\n",
    "\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd2372",
   "metadata": {},
   "source": [
    "**Single vs Double Precision Comparison:**\n",
    "\n",
    "| Property | Float32 | Float64 |\n",
    "|----------|---------|--------|\n",
    "| Machine epsilon | $\\approx 10^{-7}$ | $\\approx 10^{-16}$ |\n",
    "| Minimum achievable error | $\\approx 10^{-5}$ | $\\approx 10^{-11}$ |\n",
    "| Optimal h | $\\approx 5 \\times 10^{-3}$ | $\\approx 6 \\times 10^{-6}$ |\n",
    "| Degeneracy threshold | $\\approx 10^{-7}$ | $\\approx 10^{-16}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a47fd",
   "metadata": {},
   "source": [
    "## Problem 2 - Condition Number Analysis\n",
    "\n",
    "### Part (a)\n",
    "\n",
    "The matrix B formed from normalized basis vectors:\n",
    "\n",
    "$$B = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1+\\gamma}{\\sqrt{2+2\\gamma+\\gamma^2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2+2\\gamma+\\gamma^2}} \\end{bmatrix}$$\n",
    "\n",
    "As $\\gamma \\to 0$, the condition number grows like $\\kappa_\\infty(B) \\sim \\frac{C}{|\\gamma|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dbf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition number analysis for nearly-dependent basis\n",
    "function basis_matrix(γ)\n",
    "    norm1 = sqrt(2)\n",
    "    norm2 = sqrt(2 + 2γ + γ^2)\n",
    "    B = [1/norm1  (1+γ)/norm2;\n",
    "         1/norm1  1/norm2]\n",
    "    return B\n",
    "end\n",
    "\n",
    "# Test for various γ values\n",
    "γ_values = 10.0 .^ range(0, -10, length=50)\n",
    "cond_inf = [cond(basis_matrix(γ), Inf) for γ in γ_values]\n",
    "\n",
    "# Plot condition number vs γ\n",
    "p3 = plot(γ_values, cond_inf,\n",
    "     xscale=:log10, yscale=:log10,\n",
    "     seriestype=:scatter,\n",
    "     markersize=5, color=:blue,\n",
    "     xlabel=\"γ\",\n",
    "     ylabel=\"κ_∞(B)\",\n",
    "     title=\"Condition Number vs γ\",\n",
    "     label=\"κ_∞(B)\")\n",
    "\n",
    "# Add theoretical 1/γ line\n",
    "plot!(p3, γ_values, 2 ./ γ_values, \n",
    "      linestyle=:dash, color=:red, \n",
    "      label=\"C/γ (theoretical)\", linewidth=2)\n",
    "\n",
    "display(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30600565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ill-conditioning with a specific example\n",
    "γ_test = 1e-8\n",
    "B = basis_matrix(γ_test)\n",
    "\n",
    "println(\"For γ = \", γ_test)\n",
    "println(\"Matrix B:\")\n",
    "display(B)\n",
    "println(\"\\nDeterminant: \", det(B))\n",
    "println(\"Condition number (∞-norm): \", cond(B, Inf))\n",
    "println(\"Condition number (2-norm): \", cond(B, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a215fa7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This assignment demonstrates key concepts in numerical analysis:\n",
    "\n",
    "1. **Floating-point arithmetic** introduces rounding errors that grow as $O(\\varepsilon_M/h)$ for finite difference approximations\n",
    "\n",
    "2. **Truncation error** decreases as $O(h^2)$ for central differences\n",
    "\n",
    "3. **Optimal step size** balances these competing effects: $h_{opt} \\propto \\varepsilon_M^{1/3}$\n",
    "\n",
    "4. **Condition number** measures sensitivity to perturbations; ill-conditioned problems have large condition numbers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
